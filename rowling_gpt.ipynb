{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "2093208c",
      "metadata": {
        "id": "2093208c"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# 2 important hyperparameters important for training\n",
        "\n",
        "block_size = 32\n",
        "batch_size = 256\n",
        "\n",
        "max_iters = 2000\n",
        "learning_rate = 0.00009 # alpha\n",
        "eval_iters = 100\n",
        "dropout = 0.2 # helps the model learn better in case if there is any noise\n",
        "n_embd = 384 # size of the embedding vector\n",
        "n_head = 6 # number of heads we are running\n",
        "n_layers = 4 # number of encoder-decoder blocks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "777dfb3d",
      "metadata": {
        "id": "777dfb3d"
      },
      "outputs": [],
      "source": [
        "with open('sorcerers_stone.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "    chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "a0f19dc9",
      "metadata": {
        "id": "a0f19dc9"
      },
      "outputs": [],
      "source": [
        "# encoding corresponds to converting the character to an integer\n",
        "\n",
        "string_to_int = {ch:i for i,ch in enumerate(chars)}\n",
        "int_to_string = {i:ch for i,ch in enumerate(chars)}\n",
        "\n",
        "encode = lambda s: [string_to_int[c] for c in s]\n",
        "decode = lambda l: ''.join([int_to_string[i] for i in l])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = torch.tensor(encode(text), dtype=torch.long)"
      ],
      "metadata": {
        "id": "xHNOINy-lOZR"
      },
      "id": "xHNOINy-lOZR",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "b92f5d7f",
      "metadata": {
        "scrolled": true,
        "id": "b92f5d7f"
      },
      "outputs": [],
      "source": [
        "n = int(0.8*len(text))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "def get_batch(split):\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix]) #input batches from block\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix]) #target batches from block\n",
        "    x,y = x.to(device), y.to(device)\n",
        "    return x,y"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "502bbe7f",
      "metadata": {
        "id": "502bbe7f"
      },
      "source": [
        "creating input-output pairs for a sequence prediction task by iterating over the training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "3f89e6d0",
      "metadata": {
        "id": "3f89e6d0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bca7828-b97f-43ad-869d-5395286a6c71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "when input is tensor([32]) target is  tensor(52)\n",
            "when input is tensor([32, 52]) target is  tensor(69)\n",
            "when input is tensor([32, 52, 69]) target is  tensor(69)\n",
            "when input is tensor([32, 52, 69, 69]) target is  tensor(76)\n",
            "when input is tensor([32, 52, 69, 69, 76]) target is  tensor(2)\n",
            "when input is tensor([32, 52, 69, 69, 76,  2]) target is  tensor(40)\n",
            "when input is tensor([32, 52, 69, 69, 76,  2, 40]) target is  tensor(66)\n",
            "when input is tensor([32, 52, 69, 69, 76,  2, 40, 66]) target is  tensor(71)\n",
            "when input is tensor([32, 52, 69, 69, 76,  2, 40, 66, 71]) target is  tensor(71)\n",
            "when input is tensor([32, 52, 69, 69, 76,  2, 40, 66, 71, 71]) target is  tensor(56)\n",
            "when input is tensor([32, 52, 69, 69, 76,  2, 40, 66, 71, 71, 56]) target is  tensor(69)\n",
            "when input is tensor([32, 52, 69, 69, 76,  2, 40, 66, 71, 71, 56, 69]) target is  tensor(2)\n",
            "when input is tensor([32, 52, 69, 69, 76,  2, 40, 66, 71, 71, 56, 69,  2]) target is  tensor(52)\n",
            "when input is tensor([32, 52, 69, 69, 76,  2, 40, 66, 71, 71, 56, 69,  2, 52]) target is  tensor(65)\n",
            "when input is tensor([32, 52, 69, 69, 76,  2, 40, 66, 71, 71, 56, 69,  2, 52, 65]) target is  tensor(55)\n",
            "when input is tensor([32, 52, 69, 69, 76,  2, 40, 66, 71, 71, 56, 69,  2, 52, 65, 55]) target is  tensor(2)\n",
            "when input is tensor([32, 52, 69, 69, 76,  2, 40, 66, 71, 71, 56, 69,  2, 52, 65, 55,  2]) target is  tensor(71)\n",
            "when input is tensor([32, 52, 69, 69, 76,  2, 40, 66, 71, 71, 56, 69,  2, 52, 65, 55,  2, 71]) target is  tensor(59)\n",
            "when input is tensor([32, 52, 69, 69, 76,  2, 40, 66, 71, 71, 56, 69,  2, 52, 65, 55,  2, 71,\n",
            "        59]) target is  tensor(56)\n",
            "when input is tensor([32, 52, 69, 69, 76,  2, 40, 66, 71, 71, 56, 69,  2, 52, 65, 55,  2, 71,\n",
            "        59, 56]) target is  tensor(2)\n",
            "when input is tensor([32, 52, 69, 69, 76,  2, 40, 66, 71, 71, 56, 69,  2, 52, 65, 55,  2, 71,\n",
            "        59, 56,  2]) target is  tensor(43)\n",
            "when input is tensor([32, 52, 69, 69, 76,  2, 40, 66, 71, 71, 56, 69,  2, 52, 65, 55,  2, 71,\n",
            "        59, 56,  2, 43]) target is  tensor(66)\n",
            "when input is tensor([32, 52, 69, 69, 76,  2, 40, 66, 71, 71, 56, 69,  2, 52, 65, 55,  2, 71,\n",
            "        59, 56,  2, 43, 66]) target is  tensor(69)\n",
            "when input is tensor([32, 52, 69, 69, 76,  2, 40, 66, 71, 71, 56, 69,  2, 52, 65, 55,  2, 71,\n",
            "        59, 56,  2, 43, 66, 69]) target is  tensor(54)\n",
            "when input is tensor([32, 52, 69, 69, 76,  2, 40, 66, 71, 71, 56, 69,  2, 52, 65, 55,  2, 71,\n",
            "        59, 56,  2, 43, 66, 69, 54]) target is  tensor(56)\n",
            "when input is tensor([32, 52, 69, 69, 76,  2, 40, 66, 71, 71, 56, 69,  2, 52, 65, 55,  2, 71,\n",
            "        59, 56,  2, 43, 66, 69, 54, 56]) target is  tensor(69)\n",
            "when input is tensor([32, 52, 69, 69, 76,  2, 40, 66, 71, 71, 56, 69,  2, 52, 65, 55,  2, 71,\n",
            "        59, 56,  2, 43, 66, 69, 54, 56, 69]) target is  tensor(56)\n",
            "when input is tensor([32, 52, 69, 69, 76,  2, 40, 66, 71, 71, 56, 69,  2, 52, 65, 55,  2, 71,\n",
            "        59, 56,  2, 43, 66, 69, 54, 56, 69, 56]) target is  tensor(69)\n",
            "when input is tensor([32, 52, 69, 69, 76,  2, 40, 66, 71, 71, 56, 69,  2, 52, 65, 55,  2, 71,\n",
            "        59, 56,  2, 43, 66, 69, 54, 56, 69, 56, 69]) target is  tensor(5)\n",
            "when input is tensor([32, 52, 69, 69, 76,  2, 40, 66, 71, 71, 56, 69,  2, 52, 65, 55,  2, 71,\n",
            "        59, 56,  2, 43, 66, 69, 54, 56, 69, 56, 69,  5]) target is  tensor(70)\n",
            "when input is tensor([32, 52, 69, 69, 76,  2, 40, 66, 71, 71, 56, 69,  2, 52, 65, 55,  2, 71,\n",
            "        59, 56,  2, 43, 66, 69, 54, 56, 69, 56, 69,  5, 70]) target is  tensor(2)\n",
            "when input is tensor([32, 52, 69, 69, 76,  2, 40, 66, 71, 71, 56, 69,  2, 52, 65, 55,  2, 71,\n",
            "        59, 56,  2, 43, 66, 69, 54, 56, 69, 56, 69,  5, 70,  2]) target is  tensor(43)\n"
          ]
        }
      ],
      "source": [
        "x = train_data[:block_size]\n",
        "y = train_data[1:block_size+1]\n",
        "\n",
        "for t in range(block_size):\n",
        "    context = x[:t+1]\n",
        "    target = y[t]\n",
        "    print('when input is', context, 'target is ', target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "d4b40902",
      "metadata": {
        "id": "d4b40902"
      },
      "outputs": [],
      "source": [
        "# makes sure that pytorch doesn't make use of gradients at all\n",
        "# reduces computation and memory usage as it is not required for computing losses\n",
        "@torch.no_grad()\n",
        "\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X,Y = get_batch(split)\n",
        "            logits, loss = model(X,Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "8d45d273",
      "metadata": {
        "id": "8d45d273",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f60e800-deb8-43d1-e2da-273578c1ef9e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7.167055 M parameters\n"
          ]
        }
      ],
      "source": [
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        # each linear layer takes an input tensor of size n_embd and projects it to lower-dimensional space of size head_size\n",
        "        # key tensor is used to compute attention scores, indicating the relevance of each token in the input sequence\n",
        "        # the query tensor represents the token for which attention scores are computed\n",
        "        # the value tensor contains the actual information associated with each token\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        # initializing the lower triangular matrix, which is used to mask attention scores during self-attention to prevent tokens from attending to future tokens in the input sequence\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # input of size (batch, time-step, channels)\n",
        "        # output of size (batch, time-step, head size)\n",
        "        B,T,C = x.shape\n",
        "        # applying linear transformation on the input x\n",
        "        k = self.key(x) # (B,T,hs)\n",
        "        # same linear transformation but a different learnable transformation\n",
        "        q = self.query(x) # (B,T,hs)\n",
        "        # computing the attention scores/weights (\"affinities\")\n",
        "        # scaling with 1/sqrt(length of a row in the keys or queries matrix)\n",
        "        # transposing does the flipping of the second last dimension(-2) with the last dimension(-1)\n",
        "        # scaling is performed to normalize high dimension of the dot product between key and query\n",
        "        # this scaling ensures that no single head becomes dominant\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
        "        # applying masking - no look ahead\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        # matrix multiplication of the softmax with the value\n",
        "        v = self.value(x) # (B,T,hs)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
        "        # output is a blend of input vector values and attention placed on each token\n",
        "        return out\n",
        "\n",
        "# [1, 0, 0] - here 0 is replaced with -inf as specified above in masking\n",
        "# [1, 0.6, 0]\n",
        "# [1, 0.6, 0.4]\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "    # multiple heads of attention in parallel\n",
        "    # takes an input tensor and applies multiple attention heads in parallel, combines the outputs of these heads\n",
        "    # and then projects the concatenated output tensor back to the original embedding size\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        # a bunch of heads(num_heads=4) in parallel\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        # linear projection layer used to combine the outputs of all attention heads into a single tensor\n",
        "        # projects the concatenated multi-head output tensor into the original embedding size\n",
        "        # the purpose of linear projection is to allow the model to learn a complex combination of the information from all the heads and project it back to the original embedding size\n",
        "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
        "        # self.proj = nn.Linear()\n",
        "        # used for regularization - dropping out 20% of the neurons to avoid overfitting\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # for each attention head h, input tensor is passed through the attention head, resulting in a set of output tensors\n",
        "        # these output tensors are concatenated along the last dimension to create a single tensor with shape (B,T,F) where F is the combined size of all attention heads(num_heads*head_size)\n",
        "        # concatenating each head together, along the last dimension\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1) # (B, T, F) -> (B, T, [h1, h1, h1, h1, h2, h2, h2, h2, h3, h3, h3, h3])\n",
        "        # the concatenated output tensor is then passed through dropout for regularization, and then through the linear projection layer to reduce the size of the tensor back to the original embedding size(n_embd)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            # ReLU(x) = max(x,0)\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            # dropout makes certain percentage of the neurons to dropout and become 0 to prevent overfitting\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "# transformer block\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        # sa stands for self-attention\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        # 2 layer normalizations are applied\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # post norm is applied here as that showed better convergence\n",
        "        # refer to architecture to know more about pre and post normalization which specifies\n",
        "        # multihead attention -> norm -> feed forward -> norm  --> next block\n",
        "        y = self.sa(x)\n",
        "        x = self.ln1(x + y)\n",
        "        y = self.ffwd(x)\n",
        "        x = self.ln2(x + y)\n",
        "        return x\n",
        "\n",
        "# inherited from nn.Module class which is a base class for neural networks\n",
        "class GPTLanguageModel(nn.Module):\n",
        "\n",
        "    # this __init__ method initializes an instance of the GPTLanguageModel class\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd) # learnable parameter\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        # how many decoder blocks we have running sequentially(4)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layers)])\n",
        "        # adding at the end of the network and it is useful for the model to converge better\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    # helps training converge better\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            # initializing the weights of the linear layer from normal distribution\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, index, targets=None):\n",
        "        B, T = index.shape\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        # token embedding is similar to a dictionary - capturing meaning and relationships to other words\n",
        "        tok_emb = self.token_embedding_table(index) # (B,T,C)\n",
        "        # position embedding is similar to a map as to where the words have to be placed\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        # pos_emb = self.position_embedding_table(torch.arange(T))\n",
        "        # both dictionary and map work together to understand language fully\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        # feeding into the model/network\n",
        "        # the combined embeddings is passed through the transformer blocks\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        # the output tensor from transformer blocks is passed through a layer normalization\n",
        "        # final layer normalization\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        # Linear transformation as defined above in __init__\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        # loss calculation\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            # reshaping the logits\n",
        "            # B - batch; T - time(sequence of integers / sequence length); C - channels/vocab_size\n",
        "            B, T, C = logits.shape\n",
        "            # view is analogous to reshape in numpy\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            # reshaping the logits and targets for them to fit into the cross_entropy\n",
        "            # https://pytorch.org/docs/stable/generated/torch.nn.functional.cross_entropy.html\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    # index represents the initial context for the text generation\n",
        "    # max_new_tokens specifies the max no.of tokens to generate beyond the initial context\n",
        "    def generate(self, index, max_new_tokens):\n",
        "        # index is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            index_cond = index[:, -block_size:]\n",
        "            # get the predictions\n",
        "            # calls forward to obtain the logits and loss\n",
        "            # it returns the logits which are the raw predictions of the model before applying the softmax\n",
        "            # and logits refer to that raw and unnormalized predictions of the model\n",
        "            logits, loss = self.forward(index_cond)\n",
        "            # focus only on the last time step\n",
        "            # extracting the logits\n",
        "            # extract only the logits corresponding to the last time step of each sequence batch - 2nd index/argument\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            # -1 as we are focusing on the last dimension\n",
        "            # applying softmax activation on the logits tensor containing B and C alone\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            index_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            index = torch.cat((index, index_next), dim=1) # (B, T+1)\n",
        "        return index\n",
        "\n",
        "model = GPTLanguageModel(vocab_size)\n",
        "# print('loading model parameters...')\n",
        "# with open('model-01.pkl', 'rb') as f:\n",
        "#     model = pickle.load(f)\n",
        "# print('loaded successfully!')\n",
        "m = model.to(device)\n",
        "# context = torch.zeros((1,1), dtype=torch.long)\n",
        "# generated_chars = decode(model.generate(context, max_new_tokens=500)[0].tolist())\n",
        "# print(generated_chars)\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "09d4399a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "09d4399a",
        "outputId": "b63244ec-229c-4b23-fc96-9396a041b65c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0 - training loss: 4.4546 validation loss: 4.4554\n",
            "step 100 - training loss: 2.5112 validation loss: 2.5020\n",
            "step 200 - training loss: 2.2698 validation loss: 2.2579\n",
            "step 300 - training loss: 2.0855 validation loss: 2.0795\n",
            "step 400 - training loss: 1.9681 validation loss: 1.9649\n",
            "step 500 - training loss: 1.8773 validation loss: 1.8826\n",
            "step 600 - training loss: 1.8116 validation loss: 1.8190\n",
            "step 700 - training loss: 1.7582 validation loss: 1.7714\n",
            "step 800 - training loss: 1.7203 validation loss: 1.7315\n",
            "step 900 - training loss: 1.6837 validation loss: 1.7027\n",
            "step 1000 - training loss: 1.6506 validation loss: 1.6753\n",
            "step 1100 - training loss: 1.6157 validation loss: 1.6474\n",
            "step 1200 - training loss: 1.5966 validation loss: 1.6280\n",
            "step 1300 - training loss: 1.5720 validation loss: 1.6113\n",
            "step 1400 - training loss: 1.5482 validation loss: 1.5973\n",
            "step 1500 - training loss: 1.5368 validation loss: 1.5783\n",
            "step 1600 - training loss: 1.5173 validation loss: 1.5717\n",
            "step 1700 - training loss: 1.5055 validation loss: 1.5576\n",
            "step 1800 - training loss: 1.4923 validation loss: 1.5479\n",
            "step 1900 - training loss: 1.4762 validation loss: 1.5377\n",
            "\n",
            "tensor(1.5718, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "1.571792721748352\n"
          ]
        }
      ],
      "source": [
        "# creating a pytorch optimizer\n",
        "# defining an optimizer AdamW\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    if (iter % eval_iters == 0):\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter} - training loss: {losses['train']:.4f} validation loss: {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model.forward(xb, yb) # forward pass\n",
        "    # by default, pytorch accumulates the gradient by adding them\n",
        "    # by putting the zero_grad, we make sure that they do not add over time\n",
        "    # the previous gradients do not affect the current ones as previous gradients are from prev data\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward() # backward pass\n",
        "    optimizer.step() # letting the gradient descent work\n",
        "\n",
        "print()\n",
        "print(loss)\n",
        "print(loss.item())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cxt = encode(\"\"\"Perhaps it had something to do with living in a dark cupboard, but Harry\n",
        "had always been small and skinny for his age.\"\"\")\n",
        "context = torch.LongTensor([cxt]).to(device)\n",
        "generated_chars = decode(m.generate(context, max_new_tokens=500)[0].tolist())\n",
        "print(generated_chars)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CmD09HLbmj6r",
        "outputId": "5687a7ed-89ec-4e58-ce5e-29714fff53eb"
      },
      "id": "CmD09HLbmj6r",
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perhaps it had something to do with living in a dark cupboard, but Harry\n",
            "had always been small and skinny for his age. And thesday all conce keep yeh, leare that was so scat Goon featory, findor he heads. The read, and the probes\n",
            "fives, thought, my ning culd like behind all.\n",
            "\n",
            "\"It's -- yee hung to be looked out toward, I maging\n",
            "Toon, ockat the saw stuffully or tells, Did been hut a\n",
            "gorval down. Petwing get rest them,\" he said, whighten, shuffly.\n",
            "\n",
            "\n",
            "\"7usturieh, ro siny to mesde his lackful stud\n",
            "the withowy he loncevelly and hit wood ask at did this.\"\n",
            "\n",
            "Great stimps thard a fire, then were mornited see\n",
            "didn't wellea\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "cuda-gpt",
      "language": "python",
      "name": "cuda"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}